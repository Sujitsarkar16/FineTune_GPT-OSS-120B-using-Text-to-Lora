{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df92044-62d0-4334-9d91-173b3caf97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-OSS-120B Fine-Tuning on 2x H200\n",
    "\n",
    "## Optimizations\n",
    "\n",
    "This notebook is optimized for '2x NVIDIA H200 141GB GPUs' (282GB total GPU memory):\n",
    "\n",
    "### Hardware Configuration\n",
    "- 2x H200 141GB SXM GPUs\n",
    "- 282GB total GPU memory\n",
    "- BF16 mixed precision training\n",
    "- Flash Attention 2 enabled\n",
    "\n",
    "### Memory Optimizations\n",
    "- 4-bit quantization with bitsandbytes\n",
    "- 135GB per GPU allocation (6GB headroom)\n",
    "- Minimal CPU offloading\n",
    "- ZeRO-2 optimizer state sharding\n",
    "\n",
    "### Training Optimizations\n",
    "- Batch size: 3 per device\n",
    "- Gradient accumulation: 4 steps\n",
    "- **Effective batch size: 24** (2 GPUs × 3 × 4)\n",
    "- BF16 for H200 (better than FP16)\n",
    "- Fused AdamW optimizer\n",
    "- More frequent checkpointing\n",
    "\n",
    "### Expected Performance\n",
    "- Model loading: 4-8 minutes\n",
    "- Training (38 samples, 3 epochs): ~30-45 seconds\n",
    "- Total time: ~8-12 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04489a0a-e61c-4c5d-bf97-ff32eaa7cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.35.0\" accelerate deepspeed bitsandbytes peft sentence-transformers safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e9be2-9cfc-4957-9590-aa1ca26f9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, time, getpass, sys\n",
    "import torch, gc\n",
    "\n",
    "user = getpass.getuser()\n",
    "print(\"Current user:\", user)\n",
    "print(\"\\nInitial nvidia-smi:\")\n",
    "print(subprocess.check_output([\"nvidia-smi\", \"-L\"]).decode())\n",
    "print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "\n",
    "# parse nvidia-smi compute apps to see PIDs using GPU\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\", \"--query-compute-apps=pid,process_name,used_memory,username\", \"--format=csv,noheader,nounits\"])\n",
    "    lines = [l.strip() for l in out.decode().splitlines() if l.strip()]\n",
    "    if not lines:\n",
    "        print(\"\\nNo compute apps reported by nvidia-smi.\")\n",
    "    else:\n",
    "        print(\"\\nProcesses reported by nvidia-smi (pid, name, used_MB, username):\")\n",
    "        for l in lines:\n",
    "            print(\" \", l)\n",
    "except Exception as e:\n",
    "    print(\"Could not query compute apps:\", e)\n",
    "\n",
    "# Attempt to kill python processes owned by current user that show in ps and likely are stale.\n",
    "# WARNING: this kills user python processes. Only proceed if you started them or are sure.\n",
    "print(\"\\nListing python processes for this user (you):\")\n",
    "try:\n",
    "    ps_out = subprocess.check_output([\"ps\", \"-u\", user, \"-o\", \"pid,comm,args\"]).decode()\n",
    "    print(ps_out)\n",
    "except Exception as e:\n",
    "    print(\"ps listing failed:\", e)\n",
    "\n",
    "# Identify PIDs from nvidia-smi compute app output and try to kill them if they belong to current user\n",
    "killed_any = False\n",
    "try:\n",
    "    for l in lines:\n",
    "        parts = [p.strip() for p in l.split(\",\")]\n",
    "        if len(parts) >= 4:\n",
    "            pid_str, name, used_mb, owner = parts[0], parts[1], parts[2], parts[3]\n",
    "            try:\n",
    "                pid = int(pid_str)\n",
    "            except:\n",
    "                continue\n",
    "            if owner == user:\n",
    "                print(f\"\\nKilling PID {pid} (owner==current user) -> {name} using {used_mb} MB\")\n",
    "                try:\n",
    "                    subprocess.run([\"kill\", \"-TERM\", str(pid)], check=False)\n",
    "                    time.sleep(1)\n",
    "                    subprocess.run([\"kill\", \"-9\", str(pid)], check=False)\n",
    "                    killed_any = True\n",
    "                except Exception as e:\n",
    "                    print(\"Kill failed:\", e)\n",
    "            else:\n",
    "                print(f\"\\nSkipping PID {pid} (owner {owner} != current user).\")\n",
    "except Exception as e:\n",
    "    print(\"Parsing or kill attempt failed:\", e)\n",
    "\n",
    "# Wait and clear python-level cuda caches\n",
    "print(\"\\nWaiting 2s and clearing torch cache...\")\n",
    "time.sleep(2)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\nPost-kill nvidia-smi:\")\n",
    "print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "\n",
    "# If still memory used but no processes listed by nvidia-smi, we may need GPU reset or a runtime restart.\n",
    "# Try a GPU reset (requires sudo) - wrap in try/except since it might fail on managed providers.\n",
    "try:\n",
    "    # only attempt reset if there's leftover memory and command is available\n",
    "    nvsmi = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used --format=csv,noheader,nounits\"]).decode().strip()\n",
    "    gpus = [line.strip() for line in nvsmi.splitlines() if line.strip()]\n",
    "    leftover = any(int(line.split(\",\")[1].strip()) > 5000 for line in gpus)  # >5GB used heuristic\n",
    "    if leftover:\n",
    "        print(\"\\nAttempting per-GPU reset (requires sudo) to clear driver-held allocations. This may fail on managed hosts.\")\n",
    "        for i in range(len(gpus)):\n",
    "            try:\n",
    "                print(\"Resetting GPU\", i)\n",
    "                subprocess.run([\"sudo\", \"nvidia-smi\", \"--gpu-reset\", \"-i\", str(i)], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                print(\"Reset succeeded for GPU\", i)\n",
    "            except subprocess.CalledProcessError as ex:\n",
    "                print(\"GPU reset failed for GPU\", i, \":\", ex)\n",
    "except Exception as e:\n",
    "    print(\"GPU reset step skipped or failed (likely no sudo):\", e)\n",
    "\n",
    "print(\"\\nFinal nvidia-smi (after cleanup attempts):\")\n",
    "print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "\n",
    "if \"No running processes found\" not in subprocess.check_output([\"nvidia-smi\"]).decode():\n",
    "    print(\"\\nIf GPUs still show large allocated memory, the safe action is to restart the notebook/runtime via RunPod UI.\")\n",
    "else:\n",
    "    print(\"\\nGPUs appear free now. Proceed to model load cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62b1ca-b6e3-4eb0-b674-35d4a81440ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick fix: Clear cache and set custom cache directory\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Set cache to /tmp which usually has more space\n",
    "custom_cache = \"/tmp/hf_cache\"\n",
    "os.makedirs(custom_cache, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = custom_cache\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(custom_cache, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(custom_cache, \"datasets\")\n",
    "\n",
    "print(f\"✅ Set Hugging Face cache to: {custom_cache}\")\n",
    "\n",
    "# Check space\n",
    "try:\n",
    "    result = subprocess.check_output(['df', '-h', custom_cache]).decode()\n",
    "    print(f\"Disk space at {custom_cache}:\")\n",
    "    print(result)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear old cache if needed\n",
    "old_cache = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "if os.path.exists(old_cache):\n",
    "    try:\n",
    "        # Only remove hub cache, keep other caches\n",
    "        hub_cache = os.path.join(old_cache, \"hub\")\n",
    "        if os.path.exists(hub_cache):\n",
    "            for item in os.listdir(hub_cache):\n",
    "                if \"gpt-oss\" not in item:  # Keep current model cache\n",
    "                    item_path = os.path.join(hub_cache, item)\n",
    "                    if os.path.isdir(item_path):\n",
    "                        shutil.rmtree(item_path)\n",
    "                        print(f\"  Removed: {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Cache cleanup note: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566d466e-c597-4b33-a80b-0fd922c295c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ accelerate is installed: version 1.11.0\n",
      "⚠️  Could not verify accelerate availability\n"
     ]
    }
   ],
   "source": [
    "# Quick fix: Ensure accelerate is properly installed and imported\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    from accelerate import __version__\n",
    "    print(f\"✅ accelerate is installed: version {__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  accelerate not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"accelerate\"])\n",
    "    import accelerate\n",
    "    from accelerate import __version__\n",
    "    print(f\"✅ accelerate installed: version {__version__}\")\n",
    "\n",
    "# Additional verification\n",
    "try:\n",
    "    from accelerate.utils import is_accelerate_available\n",
    "    if is_accelerate_available():\n",
    "        print(\"✅ accelerate is available and ready for device_map\")\n",
    "    else:\n",
    "        print(\"⚠️  accelerate is installed but not available - may need restart\")\n",
    "except:\n",
    "    print(\"⚠️  Could not verify accelerate availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9c0595-5fd8-4a81-b7d1-27ce2eaa921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1072aff-8d54-46a1-83e7-01c71c2e0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ accelerate version: 1.11.0\n",
      "✅ accelerate is available for device_map support\n",
      "\n",
      "✅ Patched transformers accelerate availability\n",
      "\n",
      "Checking disk space for cache locations...\n",
      "  /tmp/hf_cache: 189.00 GB available\n",
      "  /mnt/hf_cache: 189.00 GB available\n",
      "  ./hf_cache: 84472.00 GB available\n",
      "  /workspace/hf_cache: 84472.00 GB available\n",
      "  /root/hf_cache: 189.00 GB available\n",
      "\n",
      "✅ Using cache directory: ./hf_cache (84472.00 GB available)\n",
      "PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True,max_split_size_mb:256,roundup_power2_divisions:8\n",
      "\n",
      "torch.cuda.device_count(): 2\n",
      " GPU 0: NVIDIA H200 - 139.81 GiB\n",
      " GPU 1: NVIDIA H200 - 139.81 GiB\n",
      "max_memory (optimized for 2x H200): {0: '135GiB', 1: '135GiB', 'cpu': '50GiB'}\n",
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "\n",
      "Loading model (already quantized) across 2 GPUs...\n",
      "This will take 4-8 minutes. Please wait...\n",
      "\n",
      "⚠️ flash_attn not installed: falling back to eager attention (attn_implementation='eager')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fcbebc296b454b93bf5cd2e1d9c5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully across 2 GPUs with existing quantization!\n",
      "\n",
      "Resulting hf_device_map summary:\n",
      "  0: 19 modules (example: ['model.embed_tokens', 'model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4'])\n",
      "  1: 21 modules (example: ['model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23'])\n",
      "\n",
      "Quick forward succeeded. Example output keys: ['logits', 'past_key_values']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Optimized GPU-only model loader for 2x H200\n",
    "import os, gc, time, torch, sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict, OrderedDict\n",
    "import subprocess\n",
    "\n",
    "# ===== CRITICAL: Ensure accelerate is installed and imported =====\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"✅ accelerate version: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  accelerate not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"accelerate\"])\n",
    "    import accelerate\n",
    "    print(f\"✅ accelerate installed: {accelerate.__version__}\")\n",
    "\n",
    "# Verify accelerate is working\n",
    "try:\n",
    "    from accelerate.utils import is_accelerate_available\n",
    "    if is_accelerate_available():\n",
    "        print(\"✅ accelerate is available for device_map support\\n\")\n",
    "    else:\n",
    "        print(\"⚠️  accelerate is installed but not available - may need restart\\n\")\n",
    "except ImportError:\n",
    "    # Older accelerate versions don't have is_accelerate_available\n",
    "    print(\"✅ accelerate is available for device_map support\\n\")\n",
    "\n",
    "# Force Transformers to recognise the newly installed accelerate package\n",
    "try:\n",
    "    import transformers.utils.import_utils as _transformers_import_utils\n",
    "    import transformers.modeling_utils as _transformers_modeling_utils\n",
    "    if hasattr(_transformers_import_utils, \"accelerate_available\"):\n",
    "        _transformers_import_utils.accelerate_available = True\n",
    "    if hasattr(_transformers_import_utils, \"is_accelerate_available\"):\n",
    "        _transformers_import_utils.is_accelerate_available = lambda *args, **kwargs: True\n",
    "    if hasattr(_transformers_modeling_utils, \"is_accelerate_available\"):\n",
    "        _transformers_modeling_utils.is_accelerate_available = lambda: True\n",
    "    print(\"✅ Patched transformers accelerate availability\\n\")\n",
    "except Exception as patch_err:\n",
    "    print(f\"⚠️  Could not patch transformers accelerate check: {patch_err}\\n\")\n",
    "\n",
    "MODEL_ID = \"openai/gpt-oss-120b\"   # <-- set your model id here if different\n",
    "# Set Hugging Face token - hardcoded for authentication\n",
    "HF_TOKEN = \"Add Your HF token here\"\n",
    "# Also set as environment variable for other libraries\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
    "OFFLOAD_FOLDER = \"./offload\"\n",
    "os.makedirs(OFFLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "# ===== FIX: Set custom cache directory with more space =====\n",
    "# Check available disk space and set cache to location with most space\n",
    "def get_disk_space(path):\n",
    "    \"\"\"Get free disk space in GB for a given path\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(['df', '-BG', path], stderr=subprocess.STDOUT).decode()\n",
    "        lines = result.strip().split('\\n')\n",
    "        if len(lines) >= 2:\n",
    "            parts = lines[1].split()\n",
    "            if len(parts) >= 4:\n",
    "                return float(parts[3].replace('G', ''))  # Available space in GB\n",
    "    except:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "# Try different potential cache locations\n",
    "potential_cache_dirs = [\n",
    "    \"/tmp/hf_cache\",           # Usually has more space\n",
    "    \"/mnt/hf_cache\",            # Common mount point\n",
    "    \"./hf_cache\",               # Current directory\n",
    "    \"/workspace/hf_cache\",      # Common workspace location\n",
    "    os.path.expanduser(\"~/hf_cache\"),  # Home directory alternative\n",
    "]\n",
    "\n",
    "best_cache_dir = None\n",
    "best_space = 0.0\n",
    "\n",
    "print(\"Checking disk space for cache locations...\")\n",
    "for cache_dir in potential_cache_dirs:\n",
    "    try:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        space = get_disk_space(cache_dir)\n",
    "        print(f\"  {cache_dir}: {space:.2f} GB available\")\n",
    "        if space > best_space:\n",
    "            best_space = space\n",
    "            best_cache_dir = cache_dir\n",
    "    except Exception as e:\n",
    "        print(f\"  {cache_dir}: Cannot use ({e})\")\n",
    "\n",
    "if best_cache_dir and best_space > 10:  # Need at least 10GB\n",
    "    os.environ[\"HF_HOME\"] = best_cache_dir\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(best_cache_dir, \"transformers\")\n",
    "    os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(best_cache_dir, \"datasets\")\n",
    "    print(f\"\\n✅ Using cache directory: {best_cache_dir} ({best_space:.2f} GB available)\")\n",
    "else:\n",
    "    # Fallback: try to clear old cache and use default location\n",
    "    print(\"\\n⚠️  No suitable cache location found. Attempting to clear old cache...\")\n",
    "    try:\n",
    "        default_cache = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "        if os.path.exists(default_cache):\n",
    "            # Remove old model caches (keep tokenizers)\n",
    "            import shutil\n",
    "            hub_cache = os.path.join(default_cache, \"hub\")\n",
    "            if os.path.exists(hub_cache):\n",
    "                for item in os.listdir(hub_cache):\n",
    "                    item_path = os.path.join(hub_cache, item)\n",
    "                    if os.path.isdir(item_path) and \"gpt-oss\" not in item:\n",
    "                        try:\n",
    "                            shutil.rmtree(item_path)\n",
    "                            print(f\"  Removed old cache: {item}\")\n",
    "                        except:\n",
    "                            pass\n",
    "        print(\"  Using default cache location\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Cache cleanup failed: {e}\")\n",
    "\n",
    "# Optimized CUDA memory allocation for 2x H200 (282GB total)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:256,roundup_power2_divisions:8\"\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF:\", os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])\n",
    "\n",
    "# Verify 2 GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"\\ntorch.cuda.device_count(): {num_gpus}\")\n",
    "if num_gpus < 2:\n",
    "    raise RuntimeError(f\"Expected 2 GPUs, but only {num_gpus} available. Please use a 2x H200 pod.\")\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    prop = torch.cuda.get_device_properties(i)\n",
    "    print(f\" GPU {i}: {prop.name} - {round(prop.total_memory/1024**3,2)} GiB\")\n",
    "\n",
    "# Optimized max_memory for 2x H200: use 135GB per GPU (leave 6GB headroom from 141GB)\n",
    "max_memory = {}\n",
    "for i in range(2):  # Explicitly configure for 2 GPUs\n",
    "    max_memory[i] = \"135GiB\"\n",
    "max_memory[\"cpu\"] = \"50GiB\"  # Minimal CPU offload\n",
    "print(\"max_memory (optimized for 2x H200):\", max_memory)\n",
    "\n",
    "token_kwargs = {\"use_fast\": False, \"trust_remote_code\": True}\n",
    "if HF_TOKEN:\n",
    "    token_kwargs[\"token\"] = HF_TOKEN\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **token_kwargs)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load model - model is already quantized with Mxfp4Config, so don't pass quantization_config\n",
    "print(\"\\nLoading model (already quantized) across 2 GPUs...\")\n",
    "print(\"This will take 4-8 minutes. Please wait...\\n\")\n",
    "\n",
    "# Load model with automatic device mapping across 2 GPUs\n",
    "# Decide attention implementation: try FlashAttention2, fall back to eager if unavailable\n",
    "attn_impl = \"eager\"  # default that is supported by GptOssForCausalLM\n",
    "try:\n",
    "    import flash_attn  # type: ignore\n",
    "    attn_impl = \"flash_attention_2\"\n",
    "    print(\"✅ flash_attn detected: using FlashAttention 2\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ flash_attn not installed: falling back to eager attention (attn_implementation='eager')\")\n",
    "\n",
    "# Model is already quantized, so we don't pass quantization_config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    offload_folder=OFFLOAD_FOLDER,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16,  # Use dtype instead of torch_dtype (deprecated)\n",
    "    attn_implementation=attn_impl,\n",
    "    token=HF_TOKEN if HF_TOKEN else None,\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully across 2 GPUs with existing quantization!\")\n",
    "\n",
    "# freeze params (if training with LoRA later)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# show hf_device_map summary\n",
    "device_map_res = getattr(model, \"hf_device_map\", None)\n",
    "if device_map_res:\n",
    "    placement = defaultdict(list)\n",
    "    for k,v in device_map_res.items():\n",
    "        placement[v].append(k)\n",
    "    print(\"\\nResulting hf_device_map summary:\")\n",
    "    for dev,mods in placement.items():\n",
    "        print(f\"  {dev}: {len(mods)} modules (example: {mods[:6]})\")\n",
    "else:\n",
    "    print(\"\\nhf_device_map not present (maybe model loaded on CPU or loading deferred).\")\n",
    "\n",
    "# quick test tokenization + forward pass (use tiny input)\n",
    "try:\n",
    "    inputs = tokenizer(\"Hello\", return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, output_hidden_states=False, return_dict=True)\n",
    "    print(\"\\nQuick forward succeeded. Example output keys:\", list(out.keys()))\n",
    "except Exception as e_test:\n",
    "    print(\"\\nQuick forward failed (not fatal). Exception:\")\n",
    "    import traceback; traceback.print_exc()\n",
    "    print(\"If forward failed due to OOM, model is loaded but memory insufficient for forward on GPU; consider lower batch/token length or CPU inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c6e033-cd66-4244-aacb-31205926156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.35.0 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: deepspeed in /usr/local/lib/python3.12/dist-packages (0.18.2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (1.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.1.2)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.13.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.12.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.0) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.35.0\" accelerate deepspeed bitsandbytes peft datasets sentence-transformers safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bfa377f-bfd7-4111-991c-5de71b1eb1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching LoRA with config: LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'q_proj', 'k_proj', 'v_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)\n",
      "Total params: 116,835,128,640, trainable params: 5,971,968 (0.0051%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - attach LoRA (PEFT) to the loaded model (model already present in session)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# LoRA hyperparams — tune r / alpha later\n",
    "lora_r = 8\n",
    "lora_alpha = 32\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # common for many causal LMs; adjust if your model uses different names\n",
    "lora_dropout = 0.05\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "print(\"Attaching LoRA with config:\", lora_config)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable params summary\n",
    "def print_trainable(model):\n",
    "    total, trainable = 0, 0\n",
    "    for n, p in model.named_parameters():\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        if p.requires_grad:\n",
    "            trainable += num\n",
    "    print(f\"Total params: {total:,}, trainable params: {trainable:,} ({100 * trainable/total:.4f}%)\")\n",
    "print_trainable(model)\n",
    "\n",
    "# Set dtype for LoRA params if desired\n",
    "# For stability in mixed precision training, leave main model frozen and allow LoRA params to be in fp32 or fp16 by Trainer config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0b9094-ad23-4ab7-8c0e-1a3c0cc621c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved dataset files (per split):\n",
      "  train: 46 file(s) matched. Example: ['./data/37_mapped.json', './data/48_mapped.json', './data/36_mapped.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2c4c911b164d46abe2a5e8736978e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset splits: ['train']\n",
      "Number of examples per split:\n",
      "  train: 46\n",
      "Reusing existing tokenizer object.\n",
      "Tokenizing and preprocessing dataset (single-process)...\n",
      "  Preprocessing split: train with 46 examples\n",
      "\n",
      "Tokenization complete. Splits and sizes:\n",
      "  train: 46 examples\n",
      "\n",
      "Example tokenized item (first example of train split):\n",
      "{'input_ids': tensor([117864,     13,  29844,    290, 119115,  14716,    326, 191553,  30525,\n",
      "            11,   4733,    261,   4590,  52077,   8429,    350,     64,   2086,\n",
      "             8,    395,    290,   6062,  33837,   3991,   2522,     14,  61011,\n",
      "            25,   1608,  28058,    261,   5985,  10335,    328,   4919,   4169,\n",
      "         47018,  23753,     11,   1118,    382,    261,   1899,   1604,     13,\n",
      "          5551,     11,    634,  14716,   4783,  11728,    290,   4857,   4878,\n",
      "           326,  19536,  35428,   3759,    540,    495,   3211,     13,   2214,\n",
      "          4934,     11,    481,  20323,  17800,    484, 136747,   8655,    402,\n",
      "          4169,   2171,     11,    889,    634,  30547,    673,  60592,     13,\n",
      "         65037,     11,    395,    353, 124111,    326,  11493,  17572,     11,\n",
      "           481,  13394,   5985,   7865,   7542,   1572,    290,   4857,     11,\n",
      "          7024,  23477,    326,  15652,     13,   4886,   6052,    402,  12282,\n",
      "          6411,    673,   1277, 136770,     13,   2514,   8400,     11,   5184,\n",
      "           402,  44582,   6396,    326,  10335,    290,  31687,     11,  11814,\n",
      "         35428,    326,  10089,  10674,    328,   2454,   8496,    364,  39633,\n",
      "         15461,   1239,    350,   1938,   9682,   3127,    220,   1422,     14,\n",
      "          1130,    279,  15394,    777, 155633,  14716,    326,  30525,   1402,\n",
      "            48,     16,     25,  38966,    392,  31590,      1,    472,  28989,\n",
      "           656,   2647,  41908,    328,   4305,   5269,    326,  16622,    558,\n",
      "         17045,     25,   1486,     13,     16,   1486,   5005, 126346,  40758,\n",
      "            12,    425,  22081,    382,    290,   7060,    503,   4495,    328,\n",
      "           261,   9141,  11684,   2359,    484,    382,   6977,    395,  39838,\n",
      "            13,    425,  22081,    472,    261,   6062,  88369,    483,   5618,\n",
      "           316,  36854,  27336,  41908,     13,    425,  22081,  28989,    656,\n",
      "           290,   5989,  20118,   7657,  16344,    316,    290,   5989,   4169,\n",
      "           382,    290,   2608,    306,   1118,    290,   5024,   4305,    853,\n",
      "          1339,  42324,     13,   7657,  15193,    382,  23121,    538,    290,\n",
      "          4305,    853,   4465,   2915,  34933,    326,  28668,    316,   3644,\n",
      "         18450,   4733,     13,   7657,  22081,    395,   5989,    382,    290,\n",
      "          4733,    328,    290,   4305,    484,    501,  35399,     13,    425,\n",
      "          4479,    328,  74575,  20118,   7657,   2214,    261,  24403,    290,\n",
      "          8655,    553,   1869,   2647,    591,    484,    328,    290,  15318,\n",
      "            13,   7657,    623,   4169,   9621,    395,   2395,    553,   1058,\n",
      "            11,   3097,    328,  18300,    326,    290,  10881,   1058,     13,\n",
      "          7657,   2214,    290,  24403,    480,    853,    316,   2304,   2631,\n",
      "           328,    220,     17,   6009,    328,  25433,  20118,   6240,   3325,\n",
      "           220,     18,    410,    220,     16,      8,  24798,  54301,    220,\n",
      "            17,      8,  50095,  54301,    425,  24798,  25577,    553,    290,\n",
      "          1001,    484,  10498,    395,    290,  47388,  15222,    328,    290,\n",
      "          4305,     13,    425,    623,   4169,    382,  15646, 165141,   7557,\n",
      "           290,  24798,  25577,    472,   1879,    553,    290,   3611,    395,\n",
      "           290,  18300,    328,   1888,     13,    425,  50095,  25433,    553,\n",
      "           290,   1001,    484,  12407,    290,   1888,    316,    290,   1268,\n",
      "          1825,    503,    290,  15318,    503,   5989,     13, 101822, 128468,\n",
      "         27813,  22555,     25,    220,     18,   1029,     48,     17,     25,\n",
      "         57892,    392,   8270,  22081,  11249,      1,  92312,    328, 121379,\n",
      "         20692,    558,  17045,     25,    220,     21,     13,     22,  91657,\n",
      "        126346, 171978,    425,  17565,  22081,  11249,    382,    261,   2273,\n",
      "           328,  22533,    290,   4169,    328,    261,   1888,   1819,   5890,\n",
      "          9621,     13,    425,    353, 124111,   5184,   7557,   1495,    316,\n",
      "         10635,    261,   1899,   4169,   1888,    483,   2360,    261,  19957,\n",
      "         10331,    472,   4149,     13,   6240,   3325,    220,     19,    410,\n",
      "           425,    623, 121379,  20692,    665,    413,   6177,    656,   1199,\n",
      "           328,    353, 124111,    472,    480,  26671,    316,   2575,    290,\n",
      "          1888,    402,    722,   8374,     13,  81033,    623,  23477,    553,\n",
      "         20118,    220,     16,      8,  15193,  92526,  25415,    425,    623,\n",
      "          5989,  24333,    382,    290,   1422,     13,     15, 200002]), 'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   1422,     13,     15, 200002]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# === Fixed: robust dataset loading + preprocessing + tokenization ===\n",
    "# Paste this into one notebook cell and run.\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os, glob, math, json\n",
    "\n",
    "# ----- User-configurable -----\n",
    "MODEL_ID = \"openai/gpt-oss-120b\"    # model id used to create tokenizer\n",
    "# Accept either a single file, dict, or glob pattern(s).\n",
    "# Example single file: {\"train\": \"/mnt/data/6_mapped.json\"}\n",
    "# Example glob: {\"train\": \"./data/*.json\", \"validation\": \"./val/*.json\"}\n",
    "data_files = {\"train\": \"./data/*.json\"}   # <-- change to your files or keep glob\n",
    "# -----------------------------\n",
    "\n",
    "# Validate data_files format\n",
    "if not isinstance(data_files, dict):\n",
    "    raise TypeError(\"data_files must be a dict like {'train': './data/*.json', 'validation': './val/*.json'}\")\n",
    "\n",
    "# Expand globs for visibility (not strictly required by load_dataset but helpful for debug)\n",
    "expanded = {}\n",
    "for split, pattern in data_files.items():\n",
    "    if isinstance(pattern, (list, tuple)):\n",
    "        matches = []\n",
    "        for p in pattern:\n",
    "            matches += glob.glob(p)\n",
    "    else:\n",
    "        matches = glob.glob(pattern)\n",
    "    expanded[split] = matches\n",
    "print(\"Resolved dataset files (per split):\")\n",
    "for s, files in expanded.items():\n",
    "    print(f\"  {s}: {len(files)} file(s) matched. Example: {files[:3]}\")\n",
    "\n",
    "# Basic sanity check\n",
    "if not any(expanded.values()):\n",
    "    raise FileNotFoundError(f\"No files matched any pattern in data_files: {data_files}\")\n",
    "\n",
    "# Use the original dict with patterns for load_dataset (HF supports globs)\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "print(\"Loaded dataset splits:\", list(ds.keys()))\n",
    "print(\"Number of examples per split:\")\n",
    "for k in ds:\n",
    "    print(f\"  {k}: {len(ds[k])}\")\n",
    "\n",
    "# Reuse tokenizer if available in session, else create new\n",
    "try:\n",
    "    tokenizer\n",
    "    print(\"Reusing existing tokenizer object.\")\n",
    "except NameError:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False, trust_remote_code=True)\n",
    "    print(\"Tokenizer instantiated for\", MODEL_ID)\n",
    "\n",
    "# ---------- Prompt builder & label extractor ----------\n",
    "def build_prompt_from_item(item):\n",
    "    parts = []\n",
    "    parts.append(\"You are an automated grader. Given the student's answers and rubric alignment, output a single numeric score (a number) for the whole submission).\")\n",
    "    # optional top-level context\n",
    "    if item.get(\"feedback\"):\n",
    "        parts.append(f\"Context/feedback: {item.get('feedback')}\")\n",
    "    # include optional top-level meta\n",
    "    if item.get(\"max_score\") is not None and item.get(\"score\") is not None:\n",
    "        parts.append(f\"Ground-truth (for reference): {item.get('score')}/{item.get('max_score')}\")\n",
    "    elif item.get(\"score\") is not None:\n",
    "        parts.append(f\"Ground-truth (for reference): {item.get('score')}\")\n",
    "    # include per-question rubric alignment if present\n",
    "    if \"rubric_alignment\" in item and isinstance(item[\"rubric_alignment\"], list):\n",
    "        parts.append(\"Student per-question answers and alignment:\")\n",
    "        for i, qobj in enumerate(item[\"rubric_alignment\"], start=1):\n",
    "            q = qobj.get(\"question\", \"\") or qobj.get(\"prompt\", \"\")\n",
    "            a = qobj.get(\"answer\", \"\") or qobj.get(\"student_answer\", \"\")\n",
    "            # normalize whitespace\n",
    "            q = \" \".join(str(q).split())\n",
    "            a = \" \".join(str(a).split())\n",
    "            # optionally include a per-question reference score if provided\n",
    "            qscore = qobj.get(\"score\")\n",
    "            if qscore is not None:\n",
    "                parts.append(f\"Q{i}: {q}\\nAnswer: {a}\\n(ref_score: {qscore})\")\n",
    "            else:\n",
    "                parts.append(f\"Q{i}: {q}\\nAnswer: {a}\")\n",
    "    else:\n",
    "        # fallback keys\n",
    "        if item.get(\"question\") and item.get(\"student_answer\"):\n",
    "            parts.append(f\"Question: {item.get('question')}\\nStudent answer: {item.get('student_answer')}\")\n",
    "    parts.append(\"\\nInstruction: Respond with a single numeric score (e.g., 3.5) and nothing else.\\nScore: \")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def extract_numeric_label(item):\n",
    "    # 1) explicit label\n",
    "    if \"label\" in item and item[\"label\"] is not None:\n",
    "        return float(item[\"label\"])\n",
    "    # 2) top-level score\n",
    "    if \"score\" in item and item[\"score\"] is not None:\n",
    "        return float(item[\"score\"])\n",
    "    # 3) normalized_score * max_score\n",
    "    if \"normalized_score\" in item and item[\"normalized_score\"] is not None:\n",
    "        norm = float(item[\"normalized_score\"])\n",
    "        if item.get(\"max_score\") is not None:\n",
    "            return float(norm) * float(item.get(\"max_score\"))\n",
    "        return float(norm) * 100.0\n",
    "    # 4) sum per-question scores in rubric_alignment\n",
    "    if \"rubric_alignment\" in item and isinstance(item[\"rubric_alignment\"], list):\n",
    "        s = 0.0; found = False\n",
    "        for q in item[\"rubric_alignment\"]:\n",
    "            if \"score\" in q and q[\"score\"] is not None:\n",
    "                try:\n",
    "                    s += float(q[\"score\"])\n",
    "                    found = True\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if found:\n",
    "            return s\n",
    "    # 5) try alternative keys that may exist\n",
    "    for alt in (\"final_score\", \"total_score\", \"marks\", \"grade\"):\n",
    "        if alt in item and item[alt] is not None:\n",
    "            return float(item[alt])\n",
    "    # If none found, raise clear error with keys summary\n",
    "    raise ValueError(f\"No numeric score found. Available keys: {list(item.keys())}\")\n",
    "\n",
    "# ---------- Tokenization / assembly ----------\n",
    "max_length = 512         # prompt + target truncation ceiling (tune down if OOM)\n",
    "target_max_length = 16   # short numeric target length\n",
    "\n",
    "def preprocess(example):\n",
    "    # Build prompt and extract numeric label\n",
    "    prompt = build_prompt_from_item(example)\n",
    "    label_value = extract_numeric_label(example)   # may raise, which helps debug problematic example\n",
    "    target_str = str(round(float(label_value), 2))\n",
    "    # Tokenize prompt and target separately to allow masking\n",
    "    enc_prompt = tokenizer(prompt, truncation=True, max_length=max_length, add_special_tokens=False)\n",
    "    enc_target = tokenizer(target_str, truncation=True, max_length=target_max_length, add_special_tokens=False)\n",
    "    input_ids = enc_prompt[\"input_ids\"] + enc_target[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    labels = [-100] * len(enc_prompt[\"input_ids\"]) + enc_target[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    # If too long, truncate from the left (keep the last max_length tokens)\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[-max_length:]\n",
    "        labels = labels[-max_length:]\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": [1]*len(input_ids)}\n",
    "\n",
    "# Map dataset (single-process for robust error tracebacks)\n",
    "print(\"Tokenizing and preprocessing dataset (single-process)...\")\n",
    "for split in ds:\n",
    "    print(f\"  Preprocessing split: {split} with {len(ds[split])} examples\")\n",
    "    ds[split] = ds[split].map(preprocess, remove_columns=ds[split].column_names, num_proc=1)\n",
    "\n",
    "# Set pytorch format for Trainer\n",
    "for split in ds:\n",
    "    ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "\n",
    "# Sample output summary\n",
    "print(\"\\nTokenization complete. Splits and sizes:\")\n",
    "for split in ds:\n",
    "    print(f\"  {split}: {len(ds[split])} examples\")\n",
    "print(\"\\nExample tokenized item (first example of train split):\")\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9cf060-e764-43c8-abf8-65a142f4f006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation split found — creating a 10% validation holdout from train.\n",
      "Created validation split: train=41, validation=5\n",
      "Validation split detected — eval_strategy='steps', eval_steps=50\n",
      "\n",
      "TrainingArguments created. Summary:\n",
      "  output_dir: ./t2l_lora_checkpoints\n",
      "  per_device_train_batch_size: 3\n",
      "  gradient_accumulation_steps: 4\n",
      "  num_train_epochs: 3\n",
      "  learning_rate: 0.0002\n",
      "  bf16: True\n",
      "  deepspeed: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainer object ready. Run the next cell when you're ready to fine-tune in-notebook.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4 - Build Trainer for in-notebook fine-tuning (2x H200) ===\n",
    "from datasets import DatasetDict\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import json, os\n",
    "\n",
    "assert 'ds' in globals(), \"Dataset 'ds' not found. Run the data prep cell first.\"\n",
    "assert 'model' in globals(), \"Model not loaded. Run the model loader + LoRA cells first.\"\n",
    "\n",
    "# Ensure DeepSpeed config still exists for the external script\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "ds_config_path = \"configs/ds_config.json\"\n",
    "ds_config = {\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"round_robin_gradients\": True\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"steps_per_print\": 50,\n",
    "    \"wall_clock_breakdown\": False,\n",
    "    \"zero_allow_untested_optimizer\": True\n",
    "}\n",
    "with open(ds_config_path, \"w\") as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "\n",
    "# Guarantee we have a validation split if evaluation is desired\n",
    "if \"validation\" not in ds or len(ds[\"validation\"]) == 0:\n",
    "    print(\"No validation split found — creating a 10% validation holdout from train.\")\n",
    "    split = ds[\"train\"].train_test_split(test_size=0.10, seed=42)\n",
    "    ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"]})\n",
    "    print(f\"Created validation split: train={len(ds['train'])}, validation={len(ds['validation'])}\")\n",
    "else:\n",
    "    print(f\"Existing dataset splits: train={len(ds['train'])}, validation={len(ds['validation'])}\")\n",
    "\n",
    "has_validation = \"validation\" in ds and len(ds[\"validation\"]) > 0\n",
    "output_dir = \"./t2l_lora_checkpoints\"\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=2,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=has_validation,\n",
    "    deepspeed=None,  # Keep in-notebook training single-process (device_map='auto' safe)\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    ")\n",
    "\n",
    "if has_validation:\n",
    "    training_kwargs.update({\"eval_strategy\": \"steps\", \"eval_steps\": 50})\n",
    "    print(\"Validation split detected — eval_strategy='steps', eval_steps=50\")\n",
    "else:\n",
    "    training_kwargs.update({\"eval_strategy\": \"no\"})\n",
    "    print(\"No validation split available — eval_strategy='no'\")\n",
    "\n",
    "try:\n",
    "    training_args = TrainingArguments(**training_kwargs)\n",
    "except TypeError:\n",
    "    if \"eval_strategy\" in training_kwargs:\n",
    "        training_kwargs[\"evaluation_strategy\"] = training_kwargs.pop(\"eval_strategy\")\n",
    "    training_args = TrainingArguments(**training_kwargs)\n",
    "\n",
    "print(\"\\nTrainingArguments created. Summary:\")\n",
    "for k in [\"output_dir\", \"per_device_train_batch_size\", \"gradient_accumulation_steps\", \"num_train_epochs\", \"learning_rate\", \"bf16\", \"deepspeed\"]:\n",
    "    print(f\"  {k}: {getattr(training_args, k, None)}\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"] if has_validation else None,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer object ready. Run the next cell when you're ready to fine-tune in-notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a0f27-4719-4553-934a-3beccbf68d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/12 00:39 < 00:59, 0.10 it/s, Epoch 1.29/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 5 - Run in-notebook LoRA fine-tuning ===\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "assert 'trainer' in globals(), \"Trainer not built yet. Run the previous cell first.\"\n",
    "\n",
    "# Set to a checkpoint path if you want to resume training\n",
    "resume_from_checkpoint = None\n",
    "if resume_from_checkpoint is None:\n",
    "    last_checkpoint = get_last_checkpoint(\"./t2l_lora_checkpoints\")\n",
    "    if last_checkpoint:\n",
    "        print(f\"Checkpoint detected at {last_checkpoint}. Set `resume_from_checkpoint` above to resume.\")\n",
    "\n",
    "print(\"Starting trainer.train() ...\")\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "trainer.save_model(\"./t2l_lora_checkpoints\")\n",
    "trainer.save_state()\n",
    "print(\"Training finished. Trainer metrics:\", train_result.metrics)\n",
    "\n",
    "if getattr(trainer, \"eval_dataset\", None) is not None:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Validation metrics:\", eval_metrics)\n",
    "else:\n",
    "    print(\"No validation dataset attached — skipping evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e07f9a-4893-40cd-bf0c-49e442ec1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: write train_script.py and launch with deepspeed across 2 GPUs ===\n",
    "import textwrap\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "TRAIN_SCRIPT = \"train_script.py\"\n",
    "DS_CONFIG = \"configs/ds_config.json\"\n",
    "\n",
    "# Ensure MODEL_ID and data_files are defined\n",
    "MODEL_ID = \"openai/gpt-oss-120b\"  # Always use GPT-OSS-120B, no fallback model\n",
    "\n",
    "if 'data_files' not in globals():\n",
    "    print(\"ERROR: data_files not defined. Please define it before running this cell.\")\n",
    "    print(\"Example: data_files = {'train': 'train.json', 'validation': 'val.json'}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check if deepspeed config exists before proceeding\n",
    "if not os.path.exists(DS_CONFIG):\n",
    "    print(f\"ERROR: DeepSpeed config not found at {DS_CONFIG}\")\n",
    "    print(\"Please create the config file first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Convert data_files to a string representation for the script\n",
    "data_files_str = repr(data_files)\n",
    "\n",
    "# Build the training script content\n",
    "script = f'''import os\n",
    "import math\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"{MODEL_ID}\"\n",
    "# Get Hugging Face token from environment variable or use placeholder\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"your_huggingface_token_here\")\n",
    "# Also set as environment variable\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
    "data_files = {data_files_str}\n",
    "\n",
    "# ---- tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False, trust_remote_code=True, token=HF_TOKEN)\n",
    "\n",
    "# Set padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ---- load json files (glob supported) ----\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# ---- preprocessing utils (same as in-notebook) ----\n",
    "def build_prompt_from_item(item):\n",
    "    parts = []\n",
    "    parts.append(\"You are an automated grader. Given the student's answers and rubric alignment, output a single numeric score (a number) for the whole submission.\")\n",
    "    if item.get(\"feedback\"):\n",
    "        parts.append(f\"Context/feedback: {{item.get('feedback')}}\")\n",
    "    if item.get(\"max_score\") is not None and item.get(\"score\") is not None:\n",
    "        parts.append(f\"Ground-truth (for reference): {{item.get('score')}}/{{item.get('max_score')}}\")\n",
    "    elif item.get(\"score\") is not None:\n",
    "        parts.append(f\"Ground-truth (for reference): {{item.get('score')}}\")\n",
    "    if \"rubric_alignment\" in item and isinstance(item[\"rubric_alignment\"], list):\n",
    "        parts.append(\"Student per-question answers and alignment:\")\n",
    "        for i, qobj in enumerate(item[\"rubric_alignment\"], start=1):\n",
    "            q = qobj.get(\"question\", \"\") or qobj.get(\"prompt\", \"\")\n",
    "            a = qobj.get(\"answer\", \"\") or qobj.get(\"student_answer\", \"\")\n",
    "            q = \" \".join(str(q).split())\n",
    "            a = \" \".join(str(a).split())\n",
    "            qscore = qobj.get(\"score\")\n",
    "            if qscore is not None:\n",
    "                parts.append(f\"Q{{i}}: {{q}}\\\\nAnswer: {{a}}\\\\n(ref_score: {{qscore}})\")\n",
    "            else:\n",
    "                parts.append(f\"Q{{i}}: {{q}}\\\\nAnswer: {{a}}\")\n",
    "    else:\n",
    "        if item.get(\"question\") and item.get(\"student_answer\"):\n",
    "            parts.append(f\"Question: {{item.get('question')}}\\\\nStudent answer: {{item.get('student_answer')}}\")\n",
    "    parts.append(\"\\\\nInstruction: Respond with a single numeric score (e.g., 3.5) and nothing else.\\\\nScore: \")\n",
    "    return \"\\\\n\\\\n\".join(parts)\n",
    "\n",
    "def extract_numeric_label(item):\n",
    "    if \"label\" in item and item[\"label\"] is not None:\n",
    "        return float(item[\"label\"])\n",
    "    if \"score\" in item and item[\"score\"] is not None:\n",
    "        return float(item[\"score\"])\n",
    "    if \"normalized_score\" in item and item[\"normalized_score\"] is not None:\n",
    "        norm = float(item[\"normalized_score\"])\n",
    "        if item.get(\"max_score\") is not None:\n",
    "            return float(norm) * float(item.get(\"max_score\"))\n",
    "        return float(norm) * 100.0\n",
    "    if \"rubric_alignment\" in item and isinstance(item[\"rubric_alignment\"], list):\n",
    "        s = 0.0\n",
    "        found = False\n",
    "        for q in item[\"rubric_alignment\"]:\n",
    "            if \"score\" in q and q[\"score\"] is not None:\n",
    "                try:\n",
    "                    s += float(q[\"score\"])\n",
    "                    found = True\n",
    "                except:\n",
    "                    pass\n",
    "        if found:\n",
    "            return s\n",
    "    for alt in (\"final_score\", \"total_score\", \"marks\", \"grade\"):\n",
    "        if alt in item and item[alt] is not None:\n",
    "            return float(item[alt])\n",
    "    raise ValueError(\"No numeric score found for item; keys: \" + str(list(item.keys())))\n",
    "\n",
    "# ---- tokenization/preprocessing ----\n",
    "max_length = 512\n",
    "target_max_length = 16\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = build_prompt_from_item(example)\n",
    "    label_value = extract_numeric_label(example)\n",
    "    target_str = str(round(float(label_value), 2))\n",
    "    enc_prompt = tokenizer(prompt, truncation=True, max_length=max_length, add_special_tokens=False)\n",
    "    enc_target = tokenizer(target_str, truncation=True, max_length=target_max_length, add_special_tokens=False)\n",
    "    input_ids = enc_prompt[\"input_ids\"] + enc_target[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    labels = [-100]*len(enc_prompt[\"input_ids\"]) + enc_target[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[-max_length:]\n",
    "        labels = labels[-max_length:]\n",
    "    return {{\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": [1]*len(input_ids)\n",
    "    }}\n",
    "\n",
    "print(\"Dataset splits before mapping:\", ds)\n",
    "for split in ds:\n",
    "    print(f\"Processing split: {{split}}, size: {{len(ds[split])}}\")\n",
    "    ds[split] = ds[split].map(preprocess, remove_columns=ds[split].column_names, num_proc=1)\n",
    "    ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "\n",
    "# ---- load base model ----\n",
    "print(\"Loading base model (trust_remote_code=True)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# ---- attach LoRA adapters via PEFT ----\n",
    "print(\"Attaching LoRA adapters...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ---- TrainingArguments & Trainer (optimized for 2x H200) ----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t2l_lora_checkpoints\",\n",
    "    per_device_train_batch_size=3,  # Optimized for 2x H200\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,  # 2 GPUs × 3 × 4 = 24 effective\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=2,\n",
    "    bf16=True,  # BF16 for H100\n",
    "    bf16_full_eval=True,\n",
    "    deepspeed=\"{DS_CONFIG}\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Check if validation split exists\n",
    "eval_dataset = ds.get('validation') if 'validation' in ds else None\n",
    "if eval_dataset is None:\n",
    "    print(\"Warning: No validation split found. Training without evaluation.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting trainer.train() (this will be executed under DeepSpeed by the launcher).\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./t2l_lora_saved_by_deepspeed\")\n",
    "print(\"Training complete. Saved adapters to ./t2l_lora_saved_by_deepspeed\")\n",
    "'''\n",
    "\n",
    "# Write the script file\n",
    "os.makedirs(os.path.dirname(TRAIN_SCRIPT) if os.path.dirname(TRAIN_SCRIPT) else \".\", exist_ok=True)\n",
    "with open(TRAIN_SCRIPT, \"w\") as f:\n",
    "    f.write(script)\n",
    "print(f\"✓ Wrote {TRAIN_SCRIPT}\")\n",
    "\n",
    "# Launch deepspeed with 2 GPUs\n",
    "cmd = [\"deepspeed\", \"--num_gpus=2\", TRAIN_SCRIPT]\n",
    "print(f\"Launching training with command: {' '.join(cmd)}\")\n",
    "print(f\"Using 2x H200 GPUs for optimized training\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Stream output\n",
    "proc = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    bufsize=1,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    for line in proc.stdout:\n",
    "        print(line, end=\"\")\n",
    "    proc.wait()\n",
    "    print(\"-\" * 80)\n",
    "    if proc.returncode == 0:\n",
    "        print(f\"✓ Training completed successfully (exit code: {proc.returncode})\")\n",
    "    else:\n",
    "        print(f\"✗ Training failed with exit code: {proc.returncode}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠ User interrupted; terminating deepspeed process...\")\n",
    "    proc.terminate()\n",
    "    proc.wait()\n",
    "    print(f\"Terminated. Return code: {proc.returncode}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during training: {e}\")\n",
    "    proc.terminate()\n",
    "    proc.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
